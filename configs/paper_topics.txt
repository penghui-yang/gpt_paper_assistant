1. New methodological improvements or theoretical analysis about speculative decoding. Speculative decoding is a specific method for accelerating the inference of auto-regressive models, which drafts several future tokens efficiently and then verifies them in parallel.
    - Relevant: papers that discuss specific methods like speculative decoding, or improving these methods, or analyzing them theoretically. Usually, these papers will explicitly mention speculative decoding, at least they will mention decoding.
    - Not relevant: papers about adaptation to some task. Simply following instructions or inputs are not sufficient.
2. New papers about GFlowNet. GFlowNet samples a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function.
   - Relevant: papers that discuss specific methods like GFlowNet, or improving these methods, or analyzing them. Particularly those about combining GFlowNet with large language models.
   - Not relevant: any papers that do not consider GFlowNet.
3. New methodological improvements to RLHF or instruction-following which are specific fine-tuning steps that are taken to make language models better at following user instructions across a range of tasks.
   - Relevant: papers that discuss specific methods like RLHF, or instruction-tuning datasets, improving these methods, or analyzing them. Usually these papers will explicitly mention RLHF, instruction-following or instruction-tuning.
   - Not relevant: papers about adaptation to some task. Simply following instructions or inputs are not sufficient.
4. Shows new powerful test set contamination or membership inference methods for language models. Test set contamination is the phenomenon where a language model observes a benchmark dataset during pretraining.
   - Relevant: test statistics that can detect contamination of benchmarks in language models. statistics that can provide guarantees are more interesting. membership inference methods that are general enough to apply to language models are also relevant.
   - Not relevant: any papers that do not consider language models, or that do not consider test set contamination.

In suggesting papers to your friend, remember that he enjoys papers on statistical machine learning, and generative modeling in natural language processing.
Your friend also likes learning about surprising empirical results in language models, as well as clever statistical tricks.