1. New methodological improvements or theoretical analysis about inference acceleration of large language models, especially about speculative decoding. Speculative decoding is a specific method for accelerating the inference of auto-regressive models, which drafts several future tokens efficiently and then verifies them in parallel.
    - Relevant: papers that discuss specific methods about inference acceleration of LLMs, or improving these methods, or analyzing them theoretically. Papers that explicitly mention speculative decoding are highly appreciated.
    - Not relevant: papers about adaptation to some task. Simply following instructions or inputs is not sufficient.
2. New papers about GFlowNet. GFlowNet samples a diverse set of candidates in an active learning context, with a training objective that makes them approximately sample in proportion to a given reward function.
   - Relevant: papers that discuss specific methods like GFlowNet, or improving these methods, or analyzing them. Particularly those about combining GFlowNet with large language models.
   - Not relevant: any papers that do not consider GFlowNet.
3. New methodological improvements to RLHF or instruction-following which are specific fine-tuning steps that are taken to make language models better at following user instructions across a range of tasks.
   - Relevant: papers that discuss specific methods like RLHF, or instruction-tuning datasets, improving these methods, or analyzing them. Usually, these papers will explicitly mention RLHF, instruction-following or instruction-tuning.
   - Not relevant: papers about adaptation to some task. Simply following instructions or inputs is not sufficient.
4. New improvements in tokenization or integrating tokenization with language models. Tokenization is the process of converting text into tokens, which are the smallest units of meaning for a language model. Improvements in tokenization can significantly impact the efficiency and performance of language models.
   - Relevant: papers that discuss novel methods for tokenization, improvements in existing tokenization techniques, or integrating tokenization with language models. This includes research on optimizing tokenizers for better model performance, new algorithms for tokenization, or case studies demonstrating the impact of tokenization on language model efficiency and accuracy. Papers that explore "token-free" or "tokenization-free" approaches are highly appreciated.
   - Not relevant: papers that primarily focus on applications of language models without discussing the underlying tokenization process, or papers that only mention tokenization in passing without contributing new methods or significant improvements.
5. New methodological improvements or theoretical analysis in decoding techniques for large language models (LLMs). Decoding is the process used by language models to generate text based on the input sequence and the model's learned parameters.
   - Relevant: papers that discuss novel decoding methods, improvements to existing decoding techniques, or theoretical analyses of decoding processes in LLMs. This includes research on optimizing decoding strategies for better model performance, new algorithms for decoding, or studies demonstrating the impact of different decoding methods on language model outputs. Approaches that combine decoding with other model components to enhance overall efficiency and accuracy are also relevant. Papers that explicitly mention speculative decoding are highly appreciated.
   - Not relevant: papers that primarily focus on the applications of language models without discussing the decoding process in detail, or papers that only mention decoding in passing without contributing new methods or significant improvements.

In suggesting papers to your friend, remember that he enjoys papers on statistical machine learning, and generative modeling in natural language processing.
Your friend also likes learning about surprising empirical results in language models, as well as clever statistical tricks.
